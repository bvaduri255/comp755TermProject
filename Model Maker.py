# -*- coding: utf-8 -*-
"""Checking GRADS -> Maker

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONAidUDD8z5MiBbgS5J7GTxPwvYWI8Zc
"""

pip install adversarial-robustness-toolbox

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers.legacy import Adam
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras import metrics
from art.attacks.inference.membership_inference import MembershipInferenceBlackBox
from art.estimators.classification import KerasClassifier
from art.utils import load_mnist
from tqdm import tqdm

# Load dataset
(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()

# Function to create a new model
def create_model():
    model = Sequential()
    model.add(Conv2D(filters=4, kernel_size=(5, 5), strides=1, activation="relu", input_shape=(28, 28, 1)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(filters=10, kernel_size=(5, 5), strides=1, activation="relu"))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(100, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    model.compile(optimizer=Adam(), loss=categorical_crossentropy, metrics=[metrics.categorical_accuracy])
    return model

# Function to compute gradients on a subset of data
def compute_gradients(model, x_train, y_train):
    with tf.GradientTape() as tape:
        predictions = model(x_train)
        loss = categorical_crossentropy(y_train, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    return gradients

# Function to simulate multiple clients
def simulate_clients(num_clients, x_train, y_train):
    size = len(x_train) // num_clients
    clients = []
    for i in range(num_clients):
        start, end = i * size, (i + 1) * size
        client_data = (x_train[start:end], y_train[start:end])
        clients.append(client_data)
    return clients

# Function to aggregate gradients (averaging)
def aggregate_gradients(gradients):
    avg_gradients = [np.mean([client_gradients[layer] for client_gradients in gradients], axis=0) for layer in range(len(gradients[0]))]
    return avg_gradients

# Function to apply aggregated gradients to the model
def apply_gradients(model, gradients, learning_rate):
    optimizer = Adam(learning_rate=learning_rate)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# Simulating federated learning
num_clients = 5
clients = simulate_clients(num_clients, x_train, y_train)

# Initialize global model
global_model = create_model()

# Training rounds
for round in range(8):
    client_gradients = []
    for client_data in clients:
        client_model = create_model()
        client_model.set_weights(global_model.get_weights())
        gradients = compute_gradients(client_model, *client_data)
        client_gradients.append(gradients)

    # Aggregate gradients and update global model
    new_gradients = aggregate_gradients(client_gradients)
    apply_gradients(global_model, new_gradients, learning_rate=0.01)

# Evaluate the global model
global_model.evaluate(x_test, y_test)

x_training = x_train[:int(0.7*len(x_train)), :]
y_training = y_train[:int(0.7*len(y_train)), :]

x_evals = x_test[:int(0.7*len(x_test)), :]
y_evals = y_test[:int(0.7*len(y_test)), :]

art_train_attack_x = x_train[:len(x_train) - int(len(0.7*x_train)), :]
art_eval_attack_x = x_test[:len(x_test) - int(len(0.7*x_test)), :]
art_attack_x = np.vstack([art_train_attack_x, art_eval_attack_x])

art_train_attack_y = y_train[:len(y_train) - int(len(0.7*y_train)), :]
art_eval_attack_y = y_test[:len(y_test) - int(len(0.7*y_test)), :]
art_attack_y = np.vstack([art_train_attack_y, art_eval_attack_y])

first_layer_weights = global_model.layers[0].get_weights()[0]
first_layer_biases  = global_model.layers[0].get_weights()[1]
third_layer_weights = global_model.layers[2].get_weights()[0]
third_layer_biases  = global_model.layers[2].get_weights()[1]
sixth_layer_weights = global_model.layers[5].get_weights()[0]
sixth_layer_biases  = global_model.layers[5].get_weights()[1]
seventh_layer_weights = global_model.layers[6].get_weights()[0]
seventh_layer_biases  = global_model.layers[6].get_weights()[1]


## FOR THE BELOW FILE -> CHANGE TO LOCATION ON YOUR COMPUTER

np.save("/content/drive/MyDrive/ML_Project/Update_Weights/zero_layer_weights", first_layer_weights)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/zero_layer_biases", first_layer_biases)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/two_layer_weights", third_layer_weights)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/two_layer_biases", third_layer_biases)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/five_layer_weights", sixth_layer_weights)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/five_layer_biases", sixth_layer_biases)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/six_layer_weights", seventh_layer_weights)
np.save("/content/drive/MyDrive/ML_Project/Update_Weights/six_layer_biases", seventh_layer_biases)

